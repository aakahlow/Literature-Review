# Summaries of Papers Related to the Use of Machine Learning in Computer Architecture


## Micro-architecture Related

### Branch Prediction


- **Towards a high performance neural branch predictor, Lucian N Vintan and Mihaela Iridon, International Joint Conference on Neural Networks, 1999.**

Vinton and Iridon did one of the earliest works in dynamic branch predictions using machine learning. They used neural networks with Learning Vector Quantization (LVQ) as a learning algorithm for neural networks and were able to achieve around 3% improvement in misprediction rate compared to conventional table based branch predictors.

- **Neural methods for dynamic branch prediction, Daniel A Jiménez and Calvin Lin, ACM Transactions on Computer Systems (TOCS), 2002.**

Jimenez in this paper used neural network based components, perceptrons, to perform dynamic branch prediction. Perceptrons are considered simple and more accurate in comparison to other neural learning methods. Each single branch is allocated a perceptron. The inputs to the perceptron are the weighted bits of the "global branch history shift register", and the output is the decision about branch direction. One big advantage of perceptron predictor is the fact that the size of perceptron grows linearly with the branch history size (input to the perceptron) in contrast the size of pattern history table (PHT) in PHT based branch predictors which grows exponentially with the size of branch history. Therefore, within same hardware budget, perceptron based predictor is able to get a benefit from longer branch history register. The proposed perceptron based global branch predictor achieved 36% less branch mis-predictions compared to the most accurate branch predictor of that time (McFarling hybrid predictor) given the similar hardware resources.

One of the main differences between the work of Jimenez et al and Vinton and Iridon (above two papers) is that Jimenez et al used "only history register as perceptron predictor, whereas Vinton and Iridon used the history register and the branch address as input values to LVQ and back-propagation neural predictors". LVQ has complex hardware implementation due to computations involving floating point numbers, which can significantly increase the latency of the predictor. In contrast, predictor proposed by jimenez et al used simpler training algorithm which can be implemented more efficiently.

- **Piecewise linear branch prediction, Daniel A Jiménez, ACM SIGARCH Computer Architecture News, 2005.**

One drawback of the perceptron based branch predictor introduced by Jimenez et al (summarized above) was its inability to learn the behavior of linearly inseparable branches. A boolean function is "linearly separable" if all false instances of the function can be separated from its all true instances using a hyperplane. As an example XOR is linearly inseparable and AND is linearly separable. Jimenez later (in this paper) presented piecewise linear branch predictor. This branch predictor uses a set of piecewise linear functions to predict the outcomes for a single branch. These linear functions refer to distinct historical path that lead to the particular branch instruction. Graphically, all these functions, when combined together, form a surface. In contrast to the above perceptron based predictor, this predictor can learn the behavior of linearly inseparable branches (for example it can learn the behavior of XOR function).


- **Two-level branch prediction using neural networks, Colin Egan, Gordon Steven, Patrick Quick, Rubén Anguera, Fleur Steven, and Lucian Vintan, Journal of Systems Architecture, 2003.**

Later Egan et al in this work extended the work of Vinton and Iridon and used neural networks (based on Learning vector quantization and back-propagation) as a replacement for "second level PHT" in two-level predictors while retaining the "first level history register". They demonstrated a prediction accuracy comparable to two-level predictors of the time.

- **Dynamic feature selection for hardware prediction, Alan Fern, Robert Givan, Babak Falsafi, and TN Vijaykumar, ECE Technical Reports, Purdue University, 2000.**

This work introduced decision tree based branch predictor. Decision trees allow the branch predictor to be controlled by many "processor state features". The relevant features could change at run-time without increasing linearly in size with addition of new features (compared to table based predictors), providing significant benefits over conventional table based predictors.

- **SVMs for Improved Branch Prediction, Benjamin J Culpepper and Mark Gondree, Technical Report, UC Davis, 2005.**

Culpepper and Gondree used support vector machines (SVM) to improve accuracy of branch prediction. This SVM based branch predictor performed better compared to fast-path based predictor and gshare predictor at high hardware budgets.

- **A Study on Deep Belief Net for Branch Prediction, Yonghua Mao, Junjie Shen, and Xiaolin Gui, IEEE Access, 2017.**

Mao et al. applied deep learning to branch prediction problem. The specific deep neural network used is Deep Belief Networks (DBN), which are easy to train. It is shown that the DBN predictor (based on offline learning) can reduce misprediction rate by 3-4 % on average depending on the benchmark type compared to perceptron based predictor. It is shown that the most of the benefits for DBN predictor come from its ability to reduce misprediction rate for linearly inseparable branches which perceptron being a linear classifier could not learn. It is also shown that DBN based predictor does not perform better than the TAGE predictor (considered as the best predictor till date) in most of the cases.

### Memory Scheduling

- **Self-optimizing memory controllers: A reinforcement learning approach, Engin Ipek, Onur Mutlu, José F Martínez, and Rich Caruana, ISCA, 2008.**

Ipek et al. introduced reinforcement learning based DRAM scheduling. The proposed memory controller increased utilization of memory bandwidth and showed a speed-up of 19\% for tested applications. The DRAM scheduler which acts as a reinforcement learning agent utilizes system state defined by different factors like number of read/write requests residing in the transaction queue. The actions that the agent can take include all normal commands of DRAM controller like read, write, precharge and activate. Each action results to an immediate reward and the agent (DRAM scheduler) eventually learns a policy to maximize the "cumulative long-term reward". Q-values often associated with credit assignment define what will be the eventual benefit of any action under the given state. Q-values are stored against each action-state pair and the agent tries to pick the action which will lead to the largest Q-value. Number of Q-values against each action-state pair required to store can become very lage. A learning model CMAC is used to store all Q-values to save space. A five stage hardware pipeline is introduced to calculate Q-values each processor cycle.

- **MORSE: Multi-objective reconfigurable self-optimizing memory scheduler, Janani Mukundan and Jose F Martinez, HPCA, 2012.**

Later, Mukundan and Martinez used genetic algorithms to propose MORSE (Multi-objective Reconfigurable Self-optimizing Memory Scheduler) extending Ipek et al's work. MORSE can target optimization of different metrics like performance, energy and throughput.

- **Transactional Memory Scheduling Using Machine Learning Techniques, Basem Assiri and Costas Busch, Euromicro International Conference on Parallel, Distributed, and Network-Based Processing (PDP), 2016.**

Assiri and Busch used ML algorithms to improve the performance of schedulers for transactional memories. Transactional memories are used in multicore processors to improve performance by "avoding thread synchronization and locks overhead". Assiri and Busch have improved the working of Lazy Snapshot Algorithm (an algorithm for transactional memory scheduling) using different Machine Learning algorithms like K-NN, SVM and markov models. The evaluation shows that the K-NN (K-Nearest Neighbor) performs the best (in terms of accuracy and suitability) out of the studied ML algorithms.

### Prefetching Techniques

- **Machine learning-based prefetch optimization for data center applications, Shih-wei Liao, Tzu-Han Hung, Donald Nguyen, Chinyen Chou, Chiaheng Tu, and Hucheng Zhou, In Proceedings of the Conference on High Performance, Computing Networking, Storage and Analysis, 2009.**

Liao et al. built a framework which could figure out the best possible configuration of prefetchers for data-center workloads using different machine learning algorithms like KNN, SVM, decision trees and logistic regression. The proposed framework was applied to hardware prefetchers in an Intel Core 2 CPU. Framework's predicted prefetcher configuration achieved close to optimal performance.

- **Maximizing hardware prefetch effectiveness with machine learning, Saami Rahman, Martin Burtscher, Ziliang Zong, and Apan Qasem, International Conference on High Performance Computing and Communications, 2015.**

Rahman et al. built a framework using logistic regression and decision trees to identify the best prefetcher configuration for given multithreaded code (Liao et al's work focused on serial applications only). Hardware prefetcher configuration guided by the presented machine learning framework achieved close to 96\% speed-up of optimum configuration speed-up.

- **Data Cache Prefetching with Perceptron Learning, Haoyuan Wang and Zhiwei Luo, arXiv preprintarXiv:1712.00905, 2017.**

Wang and Luo proposed perceptron based data cache prefetching. The proposed prefetcher is a two level prefetcher which uses conventional table-based prefetcher at the first level. At the second level a perceptron is used to reduce unnecessary prefetches by identifying memory access patterns. The quantified decision result of first level alongwith some information about the cache line is transferred to the second level. A set of features (that correlate with the decision of pre-fetching) is generated and it forms the input to the perceptron. The features include: prefetch distance, transition probability, "Block address bits XOR program counter", occurrence frequency and fixed input. Experimental evaluation using SPEC-CPU2006 benchmarks shows that the proposed prefetcher is able to achieve "60.64\%-83.84\%" less memory prefetch requests.


- **Semantic locality and context-based prefetching using reinforcement learning, Leeor Peled, Shie Mannor, Uri Weiser, and Yoav Etsion, ISCA, 2015.**

Peled et al. used reinforcement learning to approximate program semantic locality, which was later used to anticipate data access patterns to improve prefetching decisions. Peled et al. realized that the use of irregular data structures can result in lower temporal and spatial locality. They relied on program semantics to capture data access patterns, and used reinforcement learning to approximate semantic locality. Accesses are considered to have semantic locality if there exists a relationship between them via a series of actions. Specifically program semantics are approximated by the use of a reinforcement learning algorithm. A subset of different attributes (e.g. program counter, accesses history, branch history, status of registers, types and offsets of objects in program data structures and types of reference operations) is used to represent the current context a program. The current context is used to make prefetching predictions and the reinforcement learning algorithm finaly updates the weights of contexts depending on the usability of current prefetches. Prefetcher is implmeneted in gem5 simulator and an LLVM pass is used to generate program semantic information. Results show that the prefetcher can achieve more than 32\% improvemnt in performance beating the competing prefetchers.


- **Towards Memory Prefetching with Neural Networks: Challenges and Insights, Leeor Peled, Uri Weiser, and Yoav Etsion, arXiv preprint arXiv:1804.00478, 2018.**

Later Peled et al. used neural networks to capture semantic locality. Memory access streams alongwith a machine state is used to train neural network at run-time which predicts the future memory accesses. Evaluation of the proposed neural prefetcher using SPEC2006, Graph500 benchmarks and other hand-written kernels indicated an average speed-up of 22\% on SPEC2006 benchmarks and 5x on other kernels. Peled et al. also performed a feasibility analysis of the proposed prefetcher which shows that the benefits of neural prefetcher are outweighed by other factors like learning overhead, power and area efficiency. However, with a few more advancements in Neural network technology such overheads can be avoided making neural prefetchers more realizable.

- **Block2Vec:A Deep Learning Strategy on Mining Block Correlations in Storage Systems, Dong Dai, Forrest Sheng Bao, Jiang Zhou, and Yong Chen, 2016 45th International Conference on Parallel Processing Workshops (ICPPW).**

Dai et al. exploited deep learning techniques to propose *Block2Vec* (which is inspired by Word2Vec used in word embeding), which can find out correlations among blocks in storage systems. Such information can be used to predict the next block accesses and used to make prefetching decisions.~They introduced a new vector based representation of blocks which include a number of features that define the block.~The correlation among blocks can be found out by using the distance between block vectors. Block2Vec provides two different models to choose for training purposes. CBOW (Continuous Bag-of-Words) predicts the current block given past and future blocks and Skip-gram model predicts the past and future blocks given the current block. Block2Vec also considers the clossness in time as a feature to impact the training process to determine the block correlations. The Skip-gram model of is shown to have the best next access block prediction accuracy when compared with other accepted methods like PG (Probability Graph) and SP (Sequential Prediction).

### Cache Line Reuse

- **Neural methods for dynamic branch prediction, Daniel A Jiménez and Calvin Lin, ACM Transactions on Computer Systems (TOCS), 2002.**

Jimenez et al. took help of genetic algorithms to introduce a pseudo-LRU (least recently used) insertion and promotion machanism for cache blocks in last level caches, which could result in ~5\% speedup compared to traditional LRU (least recently used) algorithm using much less overhead. The performance of the proposed mechanism matched other contemporary techniques like DRRIP (dynamic rereference interval prediction) and PDP (protecting distance policy with less overhead.

- **Perceptron learning for reuse prediction, Elvira Teran, Zhe Wang, and Daniel A Jiménez, MICRO, 2016.**

Teran et al. applied perceptron learning algorithm (not actual perceptrons) to predict reuse of cache bkocks. Different features (like addresses of recent memory instructions and portions of address of current block) are hashed and xored with the program counter to generate an index into a counters' table. This table has weights for each feature which are then cumulated. If the sum of the weights exceed a threshold the current block is predicted to be not reused. On correct prediction, the weights are increased and they are decreased if prediction is incorrect. The proposed method is shown to have less false positives compared to other methods that results in performance improvements. This method employs various features and has a better chance of working as correlation of features
with reuse may vary across the program. However, its hardware complexity is higher compared to the other modern reuse predictors.

### Cache Configuration

- **On the finding proper cache prediction model using neural network, Songchok Khakhaeng and Chantana Chantrapornchai, In 8th International Conference on Knowledge and Smart Technology (KST), 2016.**

Khakhaeng and Chantrapornchai used perceptron based neural network to build model to predict ideal cache block size. Neural network is trained using features from address traces of NU-MiBench suite. The particular features used for training include: cache misses and size and frequency adjoining addresses which reflects temporal and spatial locality of the program. Using a cache simulator (SMPCache) and selected data mining applications from NU-MineBench suite, address traces are collected.  Neural network is trained by keeping most of the cache configuration fixed other than the block size.  

- **A Machine Learning Methodology for Cache Recommendation. Osvaldo Navarro, Jones Mori, Javier Hoffmann, Fabian Stuckmann, and Michael Hübner, International Symposium on Applied Reconfigurable Computing, 2017.**

Navarro et al. proposed a ML based methodolgy to determine the optimal cache configuration given any input application considering its effects on energy and performance. First several benchmarks profiled to collect different features like cache hits, misses, execution time and energy consumption using different cache configurations. Dynamic instruction sequences (n-grams) are generated from the programs and are used as input features for training of different classifiers. Different types of classifiers like Bayes, Functions, Lazy, Meta, RUles and Trees (modify) are used. These classifiers then map given inputs to output classes (i.e. the optimal cache configuration). The evaluation of the trained classifiers show a precision of above 99\%.

### Value Prediction

- **Exploring perceptron-based register value prediction, John Seng and Greg Hamerly, In Second Value Prediction and Value-Based Optimization Workshop, held in conjuction with ASPLOS, 2004.** 

Seng and Hamerly did one of the earliest works to present perceptron based register value predictor. Perceptron based register value predictor maintains a table of perceptrons. Particular instruction address bits are used to index into that table of perceptrons and decide which perceptron to use. Each perceptron is fed with the global history of recently committed instructions. Predictor makes an output of 1 (if the instruction is redundant i.e. it predicts that register value produced by an instruction is already in the destination register or another register in register file) or -1 (if the instruction is not redundant). On comparing the prediction with the actual output, perceptron is fed with the positive or negative feedback depending on the accuracy of the prediction. Experimental evalution of the predictor showed that for given budget perceptron based value predictor performed better than saturating counter predictor. On average, 8KB perceptron based predictor showed a speed up of 8.1\%.

- **Neural confidence estimation for more accurate value prediction, Michael Black and Manoj Franklin, In Proceedings of the 12th international conference on High Performance Computing, 2005.**

Black and Franklin proposed perceptron based confidence estimator for a value predictor. Perceptrons identify the instructions that affect the accuracy of a prediction and estimate the confidence in the prediction. Comparison of the proposed perceptron based global estimator with the "conventional local confidence estimator" shows that it results into lesser mispredictions. As discussed earlier, perceptrons are unable to learn linearly inseparable functions. In this case, linear inseparability becomes an issue "if a correct prediction on a past instruction causes the current instruction to predict correctly sometimes and incorrectly at other times." 

### GPU Microarchitecture

- **Neural acceleration for gpu throughput processors, Amir Yazdanbakhsh, Jongse Park, Hardik Sharma, Pejman Lotfi-Kamran, and Hadi Esmaeilzadeh, MICRO, 2015.**

Yazdanbakhsh et al. used neural accelerators in GPUs to do approximation of GPU code to save energy. A neural network learns the behavior of a code segment
and executes itself on a neural hardware. A compiler is used to transform GPU code to neural equivalent. With quality degradation of 2.5\%, 2.1x reduction in energy consumption and 1.9x speed up is achieved.


### Performance and Power Estimation

- **Timothy Sherwood, Erez Perelman, Greg Hamerly, and Brad Calder. 2002. Automatically characterizing large scale program behavior. In ACM SIGARCH Computer Architecture News, Vol. 30. ACM, 45–57.**

Sherwood et al. used K-means clustering to form groups/clusters of similar basic block vectors (BBV). A basic block vector (BBV) contains the frequency of occurence of all basic blocks (a basic block corresponds to a sequence of instructions with a single entry and a single exit) of a program during a particular interval of execution. As a first step, the dimension of Basic Block Vector (BBV) is reduced using random projection as the dimension of BBVs can become very large for the entire execution of the program. Then, K-means clustering algorithm is used to find groups of similar BBVs. Any distance metric (e.g. Manhattan distance) can be used to compute the distance amont the BBVs.  These groups then act as a few representative portions of the entire program and are known as *Simpoints*. These simpoints can be used to approximate performance/power for the whole program.

- **Kenneth Hoste, Aashish Phansalkar, Lieven Eeckhout, Andy Georges, Lizy K John, and Koen De Bosschere. 2006. Performance prediction based on inherent program similarity. In Parallel Architectures and Compilation Techniques (PACT), 2006 International Conference on. IEEE, 114–122.**

Hoste et al. detected similarity among programs using data from various microarchitecture-independent statistics (like instruction classes, instruction-level parallelism and register traffic, branch predictability and working set sizes) to predict performance of programs similar to reference benchmarks with known performance on specific microarchitecture. Three different techniques (Normalization, Principal Component Analysis (PCA) and Genetic Algorithms) are used to relate program similarity to performance prediction. %These techniques are applied on data matrices containing previously mentioned microarchitecture-independent characteristics for a particular benchmark which may or may not contain the performance measures.  Considering the benchmarks in the vicinity of application of interest, it's performance is estimated using the previously mentioned techniques. The technique of normalization normalizes all microarchitecture independent characteristics such that their mean is zero and variance is 1 to avoid higher weights for characteristics with larger values. Assigning higher weights for characteristics with larger values could lead to incorrect conclusion if euclidean distance is used to figure out vicinity of application of interest in benchmark space. The second technique used is an uncorrelated data extraction technique Prinicpal Component Analysis (PCA), which tries to isolate uncorrelated microarhcitecture independent characteristics as correlated characteristics would be given bigger weights by euclidean distance. The third used technique is a proposed Genetic Algorithm that considers the importance of different characteristics opposed to previous two techniques which treat all normalized charactersittics or normalized principal components with same importance while calculating euclidean distance. The genetic algorithm assigns a scaling factor to different characterisitcs depending on their importance while calculating the euclidean distance. The evaluation of the three techniques using SPEC-CPU2006 benchmarks shows that the genetic algorithm is the most accurate one to estimate program performance compared to the other ones.
Using SPEC-CPU2006 benchmarks, the evaluation shows genetic algorithm to be more accurate compared to the other two techniques.

- **O ElMoustapha, W James, Y Charles, and AD Kshitij. 2007. On the Comparison of Regression Algorithms for Computer Architecture Performance Analysis of Software Applications. In Proceedings of the First Workshop on Statistical and Machine learning approaches applied to ARchitectures and compilaTion (SMART’07).**

Yount et al. compared various machine learning (ML) algorithms with respect to their ability to analyze architectural performance of different workloads and found tree-based ML models to be the most interpretable and almost as accurate as Artificial Neural Networks (ANNs). Workload performance metrics collected on an Intel Core2 Duo system train following ML algorithms: Multi-Linear Regression, Aritificial Neural Networks (ANNs), Locally Weighted Linear Regression, Support Vector Machine (SVM) and Model Trees (M5). M5 which is a tree based ML model shows better results compared to the other techniques in terms of accuracy and interpretability. Different microarchitectural miss events related to instruction types, memory, branch predictor and TLBs normalized by the number of exeucted instructions are used to train chosen ML techniques to predict cycles per instruction (CPI) values for a particular interval of execution. 10-fold cross validation is then used to assess the accuracy of generated models. All these techniques show error %relative absolute error 
below 10\% and have high correlation coefficients. M5's accuracy is little less than that of ANNs but it is the most interpretable technique out of the studied ones. ANNS, LWR and SMOreg (SVM based model) being black box techniques do not provide enough interpretability of the output model and are not very useful in determining the individual effects of different miss-events on performance. Multi-linear regression has better interpretability, however it cannot exhibit the non-linear effects generated by interactions of different microarchitectural events used as predictors (input variables).

- **Wooseok Lee, Youngchun Kim, Jee Ho Ryoo, Dam Sunwoo, Andreas Gerstlauer, and Lizy K John. 2015. PowerTrain: A learning-based calibration of McPAT power models. In Low Power Electronics and Design (ISLPED), 2015 IEEE/ACM International Symposium on. IEEE, 189–194.**

Lee et al. proposed ML based calibration methodology called *PowerTrain* to improve accuracy of McPAT (power estimator simulator) using regression analysis. McPAT is a well-known power estimation simlator, but it is shown to have various inaccuracies due to different sources like unmodeled architectural components, discrepancy between the modeled and actual hardware components and vague configurational parameters. The proposed calibration methodology called *PowerTrain* uses power measurements on real hardware to train McPAT using regression analysis. In a two step process, the first step involves generating a regression coefficient to move total estimated power (by the model) close to the hardware power observed empirically. The second step works at lower granularity at block-level and generates coefficients for each block separately such that the overall predicted power can move closer to the actual power. Separate coefficients for static and dynamic power are generated. Non-negative least square solver is used to reduce least mean square error as normal least mean square error (LMSE) solvers (like simple linear regression) can generate negative coefficients, which might reduce the interpretability of the power equation.~Using an ARM-A15 processor based ODROID-XU3 development board and benchmarks from MiBench and SD-VBS suites for training and testing purposes, the authors have shown that the proposed calibration methodology can result into 4\% MPAE (Mean Percentage Absolute Error) compared to the 5.29\% MPAE of the baseline.

- **Matthew J Walker, Stephan Diestelhorst, Andreas Hansson, Anup K Das, Sheng Yang, Bashir M Al-Hashimi, and Geoff V Merrett. 2017. Accurate and stable run-time power modeling for mobile and embedded cpus. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems 36, 1 (2017), 106–119.**

Walker et al. used various perormance monitoring counters and ML algorithms to estimate power consumption for mobile and embedded devices. Various performance monitoring counters counting different architectural and microarchitectural events are read. Heirarchical cluster analysis is used to group different counters/events into clusters. Only events from those clusters are selected which seem to correlate highly with the consumed power. Regression analysis is combined with the other understanding of the way the CPU consumes power to build a smart model. The values of all events are multiplied by $V^2$f to capture the relationship between power and other events before performing multiple linear regression using OLS estimator. A dynamic power component which does not depend on PMC events is also added to the model. Experiments performed using different workloads show average error of 2.8\% and 3.8\% for ARM Cortex-A15 and Cortex-A7 processors to estimate power. It is also shown that the proposed stable model can achieve less percentage error to estimate power compared to other techniques.

- **Basireddy Karunakar Reddy, Matthew J Walker, Domenico Balsamo, Stephan Diestelhorst, Bashir M Al-Hashimi, and Geoff V Merrett. 2017. Empirical cpu power modelling and estimation in the gem5 simulator. In Power and Timing Modeling, Optimization and Simulation (PATMOS), 2017 27th International Symposium on. IEEE, 1–8.**

Reddy et al. studied correlation among gem5 statistics and hardware performance monitoring counters to build a model in gem5 to estimate power consumption of ARM Cortex-A15 processor. This work uses the same model built by Walker et al. but does not include all performance monitoring events as some gem5 equivalents would not be available. The used events in ML model are cycle counts, speculative instructions, L2 cache accesses, L1 instruction cache accesses and memory bus reads. It is shown that the differences between statistics of the simulator (gem5) and those of the real hardware only effect the "estimated power consumption by 10\%".

- **Wooseok Lee, Youngchun Kim, Jee Ho Ryoo, Dam Sunwoo, Andreas Gerstlauer, and Lizy K John. 2015. PowerTrain: A learning-based calibration of McPAT power models. In Low Power Electronics and Design (ISLPED), 2015 IEEE/ACM International Symposium on. IEEE, 189–194.**

Lee et al. used spline-based regression to build model for multiprocessor performance estimation from uniprocessor models to mitigate the problems associated with cycle accurate simulation of multiprocessors. The proposed technique is called "Composable Performance Regression" (CPR). The proposed technique also accounts for contention and penalty models in multiprocessor systems while estimating the performance of multiple workloads running on a multiprocessor with shared resources (such as last level cache). Their technique first estimates the performance of each single workload and then also estimates the contention behavior when a single workload runs in the presence of other workloads. Penalty model is used to combine both single-workload's performance with the estimation of possible contention for the particular workload. The experiments show median errors of approximately 4 and 6\% while predicting dual core and quad core performance from single core performance values.

- **Mihai Pricopi, Thannirmalai Somu Muthukaruppan, Vanchinathan Venkataramani, Tulika Mitra, and Sanjay Vishin. 2013. Power-performance modeling on asymmetric multi-cores. In Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2013 International Conference on. IEEE, 1–10.**

Pricopi et al. used regression analysis to propose an analytical model to estimate performance and power for heterogeneous multicore processors (HMPs). During an application run, a CPI (Cycles per Instruction) stack is built for the application using different microarchitectural events that can impact the execution time of the application. Relying on some compiler analysis results alongwith the CPI stack, performance and power can be estimated for other cores in an HMP system. Experiments performed with an ARM big.Little system indicate an intra-core prediction error below of 15\% and an inter-core prediction error of below 17\%.

- **Xinnian Zheng, Pradeep Ravikumar, Lizy K John, and Andreas Gerstlauer. 2015. Learning-based analytical cross-platform performance prediction. In Embedded Computer Systems: Architectures, Modeling, and Simulation (SAMOS), 2015 International Conference on. IEEE, 52–59.**

Zheng et al. used Lasso Linear Regression and Constrained Locally Sparse Linear Regression (CLSLR) to explore correlation among performance of same programs on different platforms to perform cross-platform performance prediction. The test case used in their work is prediction of an ARM ISA platform based on performance on Intel and AMD x86 real host systems. Benchmarks from ACM programming contest are used for training purposes. Training set is profiled and different feature vectors containing count of various events like cache misses, branch misses etc. are collected using hardware performance counters on the host platform. These multi-dimensional vectors are fed to principal component analysis (PCA) to reduce their dimensionality. To map these feature vectors to target platform performance measurements techniques of Lasso Linear Regression and Constrained Locally Sparse Linear Regression (CLSLR) are used. The reason for using Lasso in place of ordinary linear regression is it's sensitivity to measurement noise and potential to overfit. Since, non-linear relationship can also exist between host feature vectors and target performance measurements, CLSLR is introduced to take care of such non-linearities. Applyig 10-fold cross-validation shows more than 15\% error for Lasso and less than 1\% error for CLSLR suggesting existence of non-linear relationship. Prediction accuracy is evaluated using some MiBench and SD-VBS (San Diego vision benchmark suite) benchmarks showing more than 90\% accuracy on average. The authors extended this work in by proposing *LACross* framework which applies similar methodology to predict performance and power at fine granularity of phases. *LACross* is shown to have an average error less than 2\% in entire program's performance estimation, in contrast to more than 5\% error in for SD-VBS benchmark suite.

- **Adam Jundt, Allyson Cauble-Chantrenne, Ananta Tiwari, Joshua Peraza, Michael A Laurenzano, and Laura Carrington. 2015. Compute bottlenecks on the new 64-bit ARM. In Proceedings of the 3rd International Workshop on Energy Efficient Supercomputing. ACM, 6.**

Jundt et al. used a ML model named Cubist, which uses a tree of linear regression models to observe the importance of architectural blocks that have an effect on performance and power of high performance computing applications on Intel's Sandy Bridge and ARMv8 XGene processors.~All available hardware performance counters' data is collected on both platforms. To reduce the number of input varibales for the machine learning model, Principal Component Analysis and K-means clustering are used. N element vector corresponding to an input variable (composed of performance events values: examples) is computed which contains its principal components. Different clusters of input variables are found using K-means clustering. Out of each cluster only one variable is selected for further processing. These filtered input counters are used to train Cubist, which generates a set of rules. The rules are generated based on training data which decide which regression model to follow given any test data. A diverse set of applications inlcuding NAS, PARSEC, SPEC-CPU2006 and other suites is used. The trained models are shown to have a median error of 1.26\% and 7.36\% for x86 and ARM processors in predicting performance and an error of 13.9\% and 15.4\% for ARM and x86 processors to predict power. The results futher indicate that the biggest impact on performance is shown by the CPU frontend and branch predictor, while frontend and cache events carry the biggest effect for x86 processor. For power, memory and frontend events are the most important events for ARM and cache events have the biggest impact for x86 processor.

- **Giovanni Mariani, Andreea Anghel, Rik Jongerius, and Gero Dittmann. 2017. Predicting Cloud Performance for HPC Applications: a User-oriented Approach. In Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing. IEEE Press, 524–533.**

Mariani et al. used Random Forest to estimate HPC (High Performance Computing) applications' performance on clouds using hardware independent profiling of applications. Features related to types of executed operations, instruction level parallelism (ilp), register and memory traffic and library calls are collected for different parallel applications. These statistics are collected by running instrumented code which is generated by an LLVM based profiler. Features with constant values are excluded from the training model and only one of the correlated features is selected to add in the model. Performance is also measured for each application on specific cloud configuration generating tuples T(a,d,c) where a is the executed application, d is the dataset , c is the configuration and T is the wall-clock time for the execution of application. MPI implementations of NAS parallel benchmarks are used for experimentation. A statistical technique "Design of experiments" (DOE) is applied to the training data to remove inheret noise of the cloud environment. Random forests (RF) is used to train the model because of its ability to work well when the number of given features is large. %RF is a set of different regression trees, where each tree inlcudes a subset of the training data.  RF selects the features considering if they would lead to reduction in the error of the trained model. Experiments show a mean relative error of less than 15\% when the model is validated.

- **Gene Wu, Joseph L Greathouse, Alexander Lyashevsky, Nuwan Jayasena, and Derek Chiou. 2015. GPGPU performance and power estimation using machine learning. In High Performance Computer Architecture (HPCA), 2015 IEEE 21st International Symposium on. IEEE, 564–576.**

Wu et al. proposed a neural network based GPU performance and power prediction to model solve the problem of slow simulation speed of simulators to study performance/power of GPUs. The proposed model estimates performance and power consumption of applications with changes in GPU configurations. Different OpenCL kernels running on a base configuration generate performance/power numbers alongwith other performance related counter values. All these values and the used configuration act as an input for training of the ML model. Two phase ML model is used. The first phase deals with grouping of kernels using K-means clustering algorithm. Scaling surfaces which define a cluster define how the change in configuration (number of computer units, engine and memory frequency) will impact kernel's performance. The next phase involves building of classifiers to estimate the correct cluster destination for a new kernel. These classifiers are based on neural networks. The inputs to the neural network are normalized performance counter values. The events counted by the used counters include vector and scalar instruction numbers, information related to read/write bandwidth, cache statisitcs, time of processing for vector and scalar instructions and number of load/store bank conflicts. The outputs of the neural network determines which cluster the given kernel belongs to. Experimental evaluation is performed by running OpenCL kernels from different suites like ParBoil, Rodinia, SHOC, OpenDwarfs and Phoronix test suite on AMD Radeon GPU for testing purposes. 448 configurations (with eight different compute unit values, eight engine frequencies and seven memory frequencies) are used to collect counter values to train the proposed model. Then each configuration is considered as a base configuration for the testing purposes (leaving the other configurations as test points). The results shows 15\% average error in perfomance estimation and 10\% error in power estimation for a 3.3 times range of frequency, 2.9 times range of bandwidth and 8 times difference of cluster units.

- **Kenneth O’neal, Philip Brisk, Ahmed Abousamra, Zack Waters, and Emily Shriver. 2017. GPU Performance Estimation using Software Rasterization and Machine Learning. ACM Transactions on Embedded Computing Systems (TECS) 16, 5s (2017), 148.**

O'Neal et al. used various linear/non-linear regression algorithms to propose GPU performance predicting model focusing on "pre-silicon design" of GPUs using DirectX 3D workloads.~RatSim a functional simulator is used to run programs whose outputs (program counter values) are fed to build a model, which predicts the performance of the applications. GPUSim is used to generate the performance numbers (cycles per frame - CPF) needed for training of the ML model. Performed experiments use Intel Skylake GT3 GPU as a reference model which is modeled on GPUSim. Since, authors objective is to copy the entire pre-silicon design phenomena, already available GPU models are not used. The inputs to training model is a collection of program counters for each single workload. And the output of the model is a single CPF (cycles per frame) value for every program. 14 different linear and one non-linear regression models are used from diffrerent categories namely: Non-Negative Least Squares (NNLS), Ordinary Least squares (OLS), Regularization, Regularization-NNLS and Random-Forest (only non-linear). Random Forest comes out to be the most accurate model out of all the studied ones. 10-cross validation indicates an "out-of-sample error" of approximately 14\% for Random Forest.

- **Ioana Baldini, Stephen J Fink, and Erik Altman. 2014. Predicting gpu performance from cpu runs using machine learning. In Computer Architecture and High Performance Computing (SBAC-PAD), 2014 IEEE 26th International Symposium on. IEEE, 254–261.**

Baldini et al. trained two binary classifiers Nearest Neighbor with Generalized exemplars (NNGE) and Support Vector Machine (SVMs) to predict possible GPU speed-up given a run of parallel code on CPU (using OpenMP implementation). The adopted methodology dynamically profiles programs and collects a number of useful run-time features belonging to different categories like computation (arithemetic and SIMD operations), memory (load/stores) and control flow etc. GPU execution time and OpenMP execution time are also collected. This data is then used to train the previously mentioned binary classifiers (NNGE and SVM). The performed experiments using Parboil and Rodinia benchmarks suites on Intel Xeon processor with two graphics cards (ATI FirePro and Nvidia Tesla) indicate there is very little correlation between OpenMP speedup over serial execution and GPU speedup over serial exeuction. Moreover, the trained models can predict if the GPU implementation will be beneficial or not with an accuracy of 80\% approximately. It is also shown that using different trained classifiers with SVM (trained for different threshold speedups), the exact speed-up of GPU code can be predicted with 77-90\% accuracy.

- **Newsha Ardalani, Clint Lestourgeon, Karthikeyan Sankaralingam, and Xiaojin Zhu. 2015. Cross-architecture performance prediction (xapp) using cpu code to predict gpu performance. In Microarchitecture (MICRO), 2015 48th Annual IEEE/ACM International Symposium on. IEEE, 725–737.**

Ardalani et al. proposed *XAPP* (Cross Architecture Performance Prediction) technique to estimate GPU performance from CPU code using regression and bootstrap aggregating. Different features associated with the program behavior are used to train machine learning models. These features include some basic features related to ILP (instruction level parallelism), floating point operations and memory opeations and also some advanced features like shared memory bank utlization, memory coealescing and branch divergence. This work follows a two-level ML approach. At the first level, regression is used to build the base model. Features are added to this base model one by one and only those features are added which improve performance of the model above a threshold (known as the technique of "forward feature selection stepwise regression"). At the second level, ensemble prediction ("bootstrap aggregating") is used. Using applications from different benchmark suites like PARSEC, Parboil, NAS and Rodinia for training and testing of the proposed model, it is shown to have a gemetric mean error of approximately 11\% (average error of 22\%). Model is trained on Nvidia's Maxwell GPU and validated on Nvidia' Kepler GPU.


### Thread Scheduling

- **Ramazan Bitirgen, Engin Ipek, and Jose F Martinez. 2008. Coordinated management of multiple interacting resources in chip multiprocessors: A machine learning approach. In Proceedings of the 41st annual IEEE/ACM International Symposium on Microarchitecture. IEEE Computer Society, 318–329.**

Bitirgen et al. used multiple Artificial Neural Networks (ANNs) for "coordindated management" of various shared resources in multiprocessor systems to accelerate parallel workloads. The proposed scheme consists of centralized unit called *Global Manager* and multiple ANNs built for each workload running on a chip multiprocessor (CMP). Each ANN is a performance model which takes as an input the available resources and some other factors describing the program behavior (e.g. number of read/write hits and misses in L1 data cache over a specific number of last executed instructions and the portion of dirty cache ways allocated to the applications). *Global Manager* uses these performance models to decide which resource distiribution would result in the best overall performance. A single ANN is implemented in the hardware and edge weights are multiplexed at the run time to achieve 16 virtual ANNs for a 4-core CMP (4 ANNs are used for each core).
Using SPEC spec2006Web and NAS bailey1991parallel workloads, the experiments reveal that the proposed coordinated resource management technique can achieve on average 14\% speedup over fair-share management.

- **Jiangtian Li, Xiaosong Ma, Karan Singh, Martin Schulz, Bronis R de Supinski, and Sally A McKee. 2009. Machine learning based online performance prediction for runtime parallelization and task scheduling. In Performance Analysis of Systems and Software, 2009. ISPASS 2009. IEEE International Symposium on. IEEE, 89–100.**

Li et al. used ANNs to predict the performance of parallel tasks at run-time to do an optimal scheduling. ML based task scheduling perfomed 40\% better compared to the "baseline" scheduling. The proposed technique is presented using a case study of *ASpR* (adaptively scheduled R script parallelizing tool: an extension of *pR* tool ma2007automatic which is an R script parallelizing tool) framework. *ASpR* adds following new features to the pR tool: generation of task dependence graphs (TPG), modeling of performance of TPG and scheduling of tasks using a DAG (directed acyclic graph) scheduler. Since, performance modeling is done at the run time, the model keeps on retraining as new data is obtained. "Sliding window buffer" is used to keep/store a limited amount of recent data to make the design feasible. Three layered Artifical Neural Networks (ANNs) are used for performance estimation of function executions. ANN's input is the data sizes of inputs of functions and ANN's output is the estimated execution time of the function alongwith the data sizes for outputs. After the performance information is estimated by the ANN, a linear model is used to estimate the communication cost among the tasks. Cost of loops is predicted by an approach called "test driving", in which the cost of the first loop iteration once it executes is used to predict the cost of the entire loop by the master. Modified Critical Path (MCP) scheduling algorithm is modified to determine the "granularity of loop partitioning". Experiments show that the ANN+MCP proposed technique can achieve a performance within 15\% of what can be achieved by MCP-ideal.

- **Zheng Wang and Michael FP O’Boyle. 2009. Mapping parallelism to multi-cores: a machine learning based approach. In ACM Sigplan notices, Vol. 44. ACM, 75–84.**

Wang et al. used Support Vector Machine (SVM) and Artificial Neural Networks (ANNs) to propose a compiler-based technique to map OpenMP programs on multi-core processors. The proposed approach uses two different kinds of predictors: data-sensitive and data-insensitive. Generally, a parallelism mapping problem involves two steps: determining the program scalability and then the optimal scheduling. ANNs (trained using Bayesian regularization backpropagation ) are used for determination of program scalability and SVMs are used to perform scheduling of applications. Both models are fed with the program features and scalabiltiy/scheduling information to do supervised training off-line. Features related to the code, data and runtime execution of the program are used to train the algorithms. Programs from UTDSP, the NAS parallel benchmarks, and Mibench are used for profiling and testing purposes. %"Leave one out cross validation” is used for evaluation. Comparison is done with analytical and a regression model.
The experiments performed on two multicore machines, IBM Cell and Intel Xeon, indicate that the proposed approach has the ability of producing more than 96\% of the maximum performance on both of the platforms. %Moreover the proposed approach is claimed to be highly stable across applications and architectures.

- **Zheng Wang, Georgios Tournavitis, Björn Franke, and Michael FP O’boyle. 2014. Integrating profile-driven parallelism detection and machine-learning-based mapping. ACM Transactions on Architecture and Code Optimization (TACO) 11, 1 (2014), 2.**

Wang et al. later combined "profile-driven parallelism detection" and "machine-learning based mapping" to propose a semi-automated platform-independent technique to solve the problem of parallelization and mapping of parallel code to hardware execution units. Experimental results indicate that the proposed technique can perform better than the manual parallelization of the code. The results of experiments involving NAS and SPEC CPU2000 benchmarks on IBM Cell and Intel Xeon platforms have shown that the proposed technique can perform better compared to the manual parallelization of code.
Compiler static analysis is augmented with the run-time information of the program execution. A four stage process is used to detect parallelism in the code: First, the code at the level of IR (intermediate representation) is instrumented to profile and generate data accesses and control flow information. Using the profiling information a "global control and data flow graph (CDFG)" is generated. This CDFG is then used for parallelism detection. Then OpenMP annotations are used to generate parallel code. The mapping process uses ML to predict any benefits of parallel code and the best scheduling/mapping. The ML technique used is a Support Vector Machine (SVM) model which makes both of these decisions. SVM classifier builds hyper-planes in "the multi-dimensional space of program features" to distinguish between different classes. The program features which are used to train SVM classifier include: number of instructions in the intermediate representation (IR), number of load/store operations, branch operations and cache and branch misses. SVM is trained with optimal "mapping decisions", which are obtained "from a library" of standard "parallelizable loops", and other features to build hyper-planes for decision making. For new applications the program features from sequential execution are collected and then given to SVM to obtain mapping decisions and the posibility of profitability because of parallelism. Finally OpenMP annoations in code are modified depending on the results from the previous steps.

- **Tarek Helmy, Sadam Al-Azani, and Omar Bin-Obaidellah. 2015. A Machine Learning-Based Approach to Estimate the CPU-Burst Time for Processes in the Computational Grids. In Artificial Intelligence, Modelling and Simulation (AIMS), 2015 3rd International Conference on. IEEE, 3–8.**

Helmy et al. used different machine learning techniques like Support Vector Machine (SVM), K-Nearest Neighbor (KNN), Artificial Neural Network (ANN) and Decision Tree (DT) to predict the CPU-bursts lengths for different processes waiting to be executing in a computer system. The length of CPU-bursts is used by different job scheduling algorithms like "SJF (shortest job first) and SRTF (shortest remaining time first)". The used ML techniques make their predictions based on different process attributes like "jobId", "Status", "Run Time", "Submit time", "user id", "used memory" etc. The experimental results using a grid workload indidcated that KNN performed better compared to the other algorithms on the metrics of correlation coefficient and relative absolute error.

- **Nikita Mishra, John D Lafferty, and Henry Hoffmann. 2017. ESP: A Machine Learning Approach to Predicting Application Interference. In Autonomic Computing (ICAC), 2017 IEEE International Conference on. IEEE, 125–134.**

Mishra et al. used ML to predict interference among applications running on hardware with shared resources. The proposed approach called *ESP (Estimating co-Scheduled Performance)* uses regularization to predict application interference. Regularization adds structure to ML problems when features are much larger in number compared to the data points, and does so by simultaneous selection of the used model and features. The particular regularization technique used in this work is Elastic-net. Elastic-net is an improvemed regularization technique which solves the problems associated with Ridge and Lasso regression techniques which are two other known regularization techniques. Elastic-net forms groups of features such that the ones with strong correlation are "selected or rejected together". ESP divides the modeling process in two steps. Step 1 uses Elastic-net to build a linear regression model which selects a subset of important features of execution out of the given feature set. This model relies on a set of unique 409 features measured for each application's execution and predicts slowdown for "each of the co-scheduled applications". The second step uses a higher-order regression model that also involves effect of intereacting features of execution based on features determined from step 1. Complexity of higher order model is manageable as size of feature set is reduced from step 1. Experiments are performed using different applications from PARSEC, MiBench, Rodinia and SEEC suites on a platform containing SuperMICRO and Intel Xeon processors. Integration of ESP in an application scheduler on Linux systems, increases system througput by 1.25 to 1.8 times.

- **Daniel Nemirovsky, Tugberk Arkose, Nikola Markovic, Mario Nemirovsky, Osman Unsal, and Adrian Cristal. 2017. A Machine Learning Approach for Performance Prediction and Scheduling on Heterogeneous CPUs. In Computer Architecture and High Performance Computing (SBAC-PAD), 2017 29th International Symposium on. IEEE, 121–128.**

Nemirovsky et al. proposed an ANN based approach to perform scheduling on heterogeneous processors. The proposed ANN based approach is shown to have 25-31\% increase in throughput compared to a *RoundRobin* scheduler on an ARM big.Little system. Depending on applications' characteristics running on a multicore heterogeneous processor, different schedulings can lead to different overall throughputs (difference between the worst case and the best case scheduling can be as big as 71\% for different possible combinations of 4 applications for SPLASH-2 and SPEC-CPU2006 benchmarks). The proposed scheduling process consists of four steps. The first step involves collecting statistical data for each thread. Then a "next quantum behavior predictor (NQP)", which is based on ANN, estimates thread's performance for next quantum on which scheduling has to be done. Then different ANNs are used to predict thread's performance on different hardware core types (their work uses a big and a little core). In the final step, scheduler uses ANN's outputs to determine the schedule of threads that would maximize the system performance. Used statistics are normalized into ratios before feeding them as inputs to ANNs to make sure that inputs are consistent when training and validation of model are done on two different cores. For example total executed instructions can be different on big/little cores during same time slot. In total nine different inputs related to cache miss ratio and instruction mix are used. This work uses three different ANN based models: Base model uses two ANN based predictors (for large and small cores). The second model (Online model) is a copy of the base model which trains its ANNs at run time. The third model is based on Deep learning and consists of 4 hidden-layers, where each layer has 20 hidden units (compared to 1 hidden layer with each having 6 hidden units in the base model) and also learns online. The "Deep" model's purpose is to exhibit hte best possible accuracy using ANN. Experiments performed using SPLASH-2 and SPEC2006 workloads indicate that Deep model's overhead is 4 and 5\% for SPEC2006 workloads and 4 and 3\% for SPLASH-2 workloads for ANNs of large and small core types respectively. Moreover the computational overheads of the online predictor is shown to be approximately 0.5\% per prediction.


### Design Space Exploration

- **Engin Ïpek, Sally A McKee, Rich Caruana, Bronis R de Supinski, and Martin Schulz. 2006 Efficiently exploring architectural design spaces via predictive modeling. Vol. 40. ACM.**

 Ipek et al. used neural networks to study big design spaces using regression modeling. This paper proposed a methodology which starts with selecting important design features. Simulations of training set of applications are performed for random configurations. The inputs and outputs from simulations are normalized. "Nominal paramters" (parameters that do not take numerical values like coherence policy in multiprocessors) are encoded using "one-hot encoding". "Cardinal paramters" (that take numerical valeus) after normalization have values in range of 0-1. The data set is partitioned into k folds and each fold trains a single neural network using k-fold cross validation. %Each data point is presented to the ANNs at "a frequency proportional to the inverse of its IPC" during the training process. Error's average and standard deviation are calculated based on cross validation process. The proposed model uses an ensemble of ANNs, where different points in "the parameter space" are predicted by puting the "parameters at the input layers" of different ANNs and finally an average is taken from estimations of all models.
The proposed approach can also work with other techniques like Simpoints to further reduce time to explore large design spaces compared to simulation methodologies. Using SPEC 2000 benchmarks with reduced input sizes, it is shown that the proposed model has only 1-2\% error in prediction of IPC values. %The training samples constitute 2\% of the entire design space.

- **Benjamin C Lee and David M Brooks. 2007. Illustrative design space studies with microarchitectural regression models. In High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on. IEEE, 340–351.**

Lee et al. realized that the metric and design diversity asks for an extensive design space exploration and cycle accurate simulations can become too expensive given the size of design space. They used regression modeling for fast design space exploration. The predicted measures are performance and power and used predictors are different configurational parameters. Using "domain specific knowledge" new predictors are added which cater for the interaction of two dependent predictors (for example a product of pipeline depth and L2 cache size serves as new feature carrying the interaction of two parameters). "Restricted cubic splines" are used to account for existing non-linearity in data, which partition the "predictor domain" into small pieces and "different cubic polynomials" are used for those small pieces. The model is trained on n observations which consist of architectural parameters (related to pipeline depth/width, cache sizes) and predicted values of power and performance gained by PowerTimer and Turandot simulators. The built models are then applied to "pareto frontier in power-delay space, pipeline depth, and multiprocessor heterogeneity analyses."

- **Salman Khan, Polychronis Xekalakis, John Cavazos, and Marcelo Cintra. 2007. Using predictivemodeling for cross-program design space exploration in multicore systems. In Proceedings of the 16th International Conference on Parallel Architecture and Compilation Techniques. IEEE Computer Society, 327–338.**

Khan et al. proposed feed-forward ANN-based predictive modeling technique to do design space exploration for chip multiprocessors. Previous techniques needed all benchmarks to be included for training the model (which would predict performance for an unknown configuration). The proposed model can predict for unknown configurations using unseen benchmarks as well unlike previous works that could not work for unseen benchmarks. A set of base configurations is randomly selected which are termed as *cannonical configurations*. Performance numbers (Energy-delay) for all programs are collected on these selected cannonical configuratoins. These numbers are called *Reactions*. The input vector to train the ML model consists of *Reactions* for all cannonical configurations along with the architectural parameters for that particular configuration.
When the model has to predict for an unseen application, the performance numbers first need to be collected on known cannonical configuraitons for that unseen application. These reactions alongwith new to-test configuration are given to the model so that it can predict performance/energy-delay numbers. The specific ML model used in this work is a "feed forward ANN". Regularization is applied using a bayesian technique to do feature and selection to solve the problem of overfiting.~To evaluate the proposed methodology, parallel applications from SPLASH-2 and SPEC-CPU2000 (INT) suite (these are parallelized with the help of "thread-level speculation (TLS) technique") are used. Different architectural parameters like (issue-fetch width, branch predictor and ROB configurations) are used to train the model. To do "cross program prediction for new configurations", energy-delay prediction show error of approximately 4\% using only 4 *Reactions*. For performance the average error is 2.5\% using 4 *Reactions*.


- **Wenhao Jia, Kelly A Shaw, and Margaret Martonosi. 2012. Stargazer: Automated regression-based GPU design space exploration. In Performance Analysis of Systems and Software (ISPASS), 2012 IEEE International Symposium on. IEEE, 2–13.**

Jia et al. proposed *StarGazer*, an automated framework which uses a stepwise regression algorithm to explore GPU design space by using only few samples out of all possible design points and estimates the performance of design points with only 1.1\% error on average. It explores the interactions between chosen parameters. After considering all possible parameters with possible values the proposed framework selects few sample points and collects performance measurements for those samples using any simulator (GPGPU-Sim is used in their work), a stepwise regression algorithm is applied. This repetitive process starts with a parameter that individually would result in the highest $R^2$ value for the regression equation. Then new paramters are added one by one only if they increase the $R^2$ value of the new regression model by a threshold. When new parameters are added, their interactions are also considered and added to the model if they are useful. Only "pairwise interactions" are considered in this work. Different parameters are used like ( number of blocks executing in parallel, shared memory ports, SIMD width, number of MSHR, "intra/inter warp memory coalesce", "constant and texture cache size"). Experiments invloving Rodinia benchmarks indicate that "SIMD width" is the most important factor in most of the test points, and "block concurrency" is usually the second most important factor.

- **Brandon Reagen, José Miguel Hernández-Lobato, Robert Adolf, Michael Gelbart, Paul Whatmough, Gu-Yeon Wei, and David Brooks. 2017. A case for efficient accelerator design space exploration via Bayesian optimization. In Low Power Electronics and Design (ISLPED, 2017 IEEE/ACM International Symposium on. IEEE, 1–6.**

Reagen et al. used ML (Bayesian optimization) to explore design space for a "Deep Neural Network Hardware Accelerator". Different design parameters studied in this work, like neurons/layer for DNN, L2 regularization for DNN, learning rate of DNN, loop parallelism in hardware, and hardware pipelining, have "complex interactions" among them. For example, loop parallelism determines the amount of parallel hardware which "is directly related to the number of" neurons that can be processed by DNN in parallel. All those parameters can affect the required objectives of the design: increased accuracy and energy efficiency. The aquisition function (AF) used with BayesOpt is Predictive Entropy Search Acquisition function. The proposed design flow is based on three steps. In step 1, Spearmint (a package to do bayesian optimization) chooses values of all used parameters (total fourteen in number) and performs BayesOpt. In step 2, the selected parameters are fed to DeepNet (GPU lib for training DNNs) and Aladin (accelerator simulator) to train DNN and collect energy numbers. In step 3, results from tools used in step 2 are fed back to Spearmint, which again computes the Acquistion Function (AF). BayesOPt is also compared with two other methods: Stochastic Grid search (SGS) and genetic algorithms. Stochastic Grid Search suffers from poor accuracy. Genetic algorithm is able to achieve the objective of energy efficiency, but the accuracy achieved by its design also suffers from poor accuracy. BayesOpt does better than the other two methods. Specifically SGS based designs achieve an error of approximately 61\% with energy of 14.2uJ, GA error of 28\% and energy of 4.29uJ, BayesOpt  error of 14\% and energy of 3.95uJ. %Also , "For every SGS and GA Pareto point, there is at least one point on the BayesOpt Pareto frontier wh ich dominates it in both energy and accuracy"

### Energy/Power Improvements

- **Michael Moeng and Rami Melhem. 2010. Applying statistical machine learning to multicore voltage & frequency scaling. In Proceedings of the 7th ACM international conference on Computing frontiers. ACM, 277–286.**

This paper used Decision Trees to propose a DVFS policy that will be able to predict clock frequency resulting into the least amount of energy consumption in a multicore processor. The specific goal metrics that the Decision Tree based model tries to improve are energy per user instruction (epui) and {energy-per-user-instruction}^2 (epui^2). Different measurement metrics are used to train the decision tree including: cycles, user instructions, total instructions, L1 accesses and misses, L2 accesses, misses and stalls for a DVFS interval during execution. The proposed tree is also implemented in hardware to speed up the decision tree lookup time. The experiments indicate that the suggested approach can improve epui^2 of the entire system by 14.4\% compared to a baseline system that does not employ any DVFS and by 11.3\% compared to a system that uses a greedy policy for a DVFS interval of 1ms.


- **Hwisung Jung and Massoud Pedram. 2010. Supervised learning based power management for multicore processors. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems 29, 9 (2010), 1395–1408.**

Jung and Pedram used Bayesian classification to tune voltage and frequency settings to reduce system's energy consumption in a multicore processor. The proposed power manager (PM) uses Bayesian classifier, which predicts power and performance and uses that prediction to manage power based on a lookup table (filled with already computed policy decisions). Bayesian classifier does its prediction based on a number of input features like task features and condition of service queue (SQ).

- **Ryan Cochran, Can Hankendi, Ayse K Coskun, and Sherief Reda. 2011. Pack & Cap: adaptive DVFS and thread packing under power caps. In Proceedings of the 44th annual IEEE/ACM international symposium on microarchitecture. ACM, 175–185.**

Cochran et al. showed an average energy consumption reduction of 51.6% for PARSEC workloads on a quadcore processor by proposing an approach called "Pack and Cap" that optimizes performance under a given power budget. The proposed approach uses multinomial logistic regression to build a model using performance monitoring counters (L1-regularization picks the relevant counters) to choose DVFS settings in a homogeneous CMP setting.


- **Da-Cheng Juan and Diana Marculescu. 2012. Power-aware performance increase via core/uncore reinforcement control for chip-multiprocessors. In Proceedings of the 2012 ACM/IEEE international symposium on Low power electronics and design. ACM, 97–102.**

Juan and Marculescu proposed a reinforcement learning based DVFS technique with the goal of achieving best performance under given power constraints. The suggested technique divides the central agent in many "distributed" agents and uses a supervisor for coordination among other agents to "achieve the required goal of maximizing performance under given power budget". This keeps state complexity to be linear with the number of cores. Using parallel workloads, authors have shown that the proposed technique can achieve 10.9\% increase in performance given the power restrictions.


- **Xiao Ma, Peng Huang, Xinxin Jin, Pei Wang, Soyeon Park, Dongcai Shen, Yuanyuan Zhou, Lawrence K Saul, and Geoffrey M Voelker. 2013. eDoctor: Automatically Diagnosing Abnormal Battery Drain Issues on Smartphones. In NSDI, Vol. 13. 57–70.**

Ma et al. used ML to diagnose "Abnormal Battery Drain (ABD)" on a smart phone, caused by certain applications. The proposed solution "eDoctor" relies on behavior of
execution phases, which are determined using k-means clustering algorithm. eDoctor also suggests remedial measures to the user once an ABD problem is diagnosed. Phases are determined based on resource (like network, display, processor) usage. Two different schemes "RTV (Resource Type Vector)" and "RUV (Resource Usage Vector)" are used to  distinguish one interval of execution from the other and then make phases out of the similar intervals. RTV and RUV are vectors where each component corresponds to type of resource used or the usage of that particular resource. k-means clustering is used to determine phases from the previously defined intervals. eDoctor is composed of 4 blocks: Information collector keeps track of applications' resources (sensors, Display, GPU, CPU) using RTV or RUV scheme, energy consumption and other related events like configuraiton and application updates. Data Analyzer parses the data obtained from previous block. It generates a phase table which stores the information related to each major phase of the application during the time interval between two charging times. A *Diagnosis Engine} uses Data Analyzer and Information collector's records to figure out the problematic applcation and events if any ABD issue is noticed. Finally, a *Repair Advisor} suggests the appropriate steps to mitigate the ABD issues such as uninstalling apps, terminating them after use or reverting configuration changes. The assessment study involving eDoctor shows it can diagnose 47 out of 50 cases sucessfully.


- **Rong Ye and Qiang Xu. 2014. Learning-based power management for multicore processors via idle period manipulation. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems 33, 7 (2014), 1043–1055.**

Ye and Xu proposed a learning based dynamic power management (DPM) technique for multicore processors. The proposed technique assigns tasks to cores which result into better power vs performance tradeoff. Core temperature is also added in the proposed framework to include its effect on power consumption. Q-learning which is a reinforcement learning algorithm (Q-learning decides the actions to be taken to maximize the expected reward given the current system state) is used  in the proposed DPM technique.

- **Jae-Yeon Won, Xi Chen, Paul Gratz, Jiang Hu, and Vassos Soteriou. 2014. Up by their bootstraps: Online learning in artificial neural networks for CMP uncore power management. In High Performance Computer Architecture (HPCA), 2014 IEEE 20th International Symposium on. IEEE, 308–319.**

Won et al. used ANNs to predict "uncore (Network on Chip and Last level caches) utility patterns" to increase its energy efficiency. ANN is helped by a Proportional Integral (PI) controller during the training phase. Proportional integral controllers which were also used by Chen et al. are used to help in training of ANNs. They are good in adapting to "short term system changes", however they lack in their ability to adapt to longterm system changes. Won et al. have adopted the critical latency metric first presented in. This metric is a product of uncore latency and criticality factor (product of "private cache miss rate" and load to total instructions ratio). A 3-layer neural network uses m*n inputs (m critical latency factors for each m history intervals for n cores). Total "number of outputs" of the neural network is equal to the possible voltage/frequency levels. The used nueral network uses a gaussian function as an activation function and a common back propagation function as a learning function. The proposed framework uses three control schemes that use ANN and PI (proportional integral) in tandem: "ANN-Centric Tandem Control", "Eager Tandem Control" and "Credit-Based Tandem Contorl". Experiments involving full system simulations using gem5 full system simulator, GARNET network simulator and a 16-core CMP target with NOC topology of 4 x 4 2D mesh indicate a 27\% improvement in energy-delay product of the uncore system over other competitive techniques (only PI control). A microcontroller based Power Control Unit (PCU) is used in the hardware to analyze network packets (which contain critical latency measurements encoded by each core). The ANNs are implemented in software in PCU and other control decisions are also made in this software.


- **Sheng Yang, Rishad A Shafik, Geoff V Merrett, Edward Stott, Joshua M Levine, James Davis, and Bashir M Al-Hashimi. 2015. Adaptive energy minimization of embedded heterogeneous systems using regression-based learning. In Power and Timing Modeling, Optimization and Simulation (PATMOS), 2015 25th International Workshop on. IEEE, 103–110.**

They used regression to build application-specific energy-performance tradeoff model among use of different computing resources in a heterogeneous embedded systems. The proposed model includes the effect that change in voltage and frequency would bring to the latency and current of the used resources in heterogeneous system. The decision of mapping of application on computing resources is made at the run time based on minimum energy under given performance constraints. The evaluation of the proposed approach using image processing applications on ARM cores based emebedded board indicates that this technique can achive more than 70\% energy savings in some cases in comparison to other techniques of the time.


- **Shibo Wang and Engin Ipek. 2016. Reducing data movement energy via online data clustering and encoding. In Microarchitecture (MICRO), 2016 49th Annual IEEE/ACM International Symposium on. IEEE, 1–13.**

Wang and Ipek proposed an online data clustering based technique to reduce energy of data transfers in memory. Data interconnects are often designed in such a way that they present asymmetric transmission costs (e.g. cost of transmitting a '0' is much lower than transmitting a '1'). Wang and Ipek propsed a data encoding technique based on clustering techniques that tries to reduce the number of (ones) 1s in the data to minimise its transmission cost. The proposed scheme dyanmically groups "similar data blocks into clusters". Each data block is encoded as an XOR between centre of the nearest cluster and "a sparse residual". Evaluation of the proposed technique for DDR4, LPDDR3 and last level cache indicates energy savings of 5\%., 9\% and 12\% respectively. It is also shown that the proposed technique performs better than "two dimensional bus invert (CAFO) coding, and recent value encoding"

### Hardware Security

- **Meltem Ozsoy, Khaled N Khasawneh, Caleb Donovick, Iakov Gorelik, Nael Abu-Ghazaleh, and Dmitry Ponomarev. 2016. Hardware-Based Malware Detection Using Low-Level Architectural Features. IEEE Trans. Comput. 65, 11 (2016), 3332–3344.**

Ozosoy et al. used neural network and logistic regression for detection of malware using hardware architectural features (like memory accesses and instruction mixes). FPGA based implementations of both ML models (LR and NN) are evaluated as well.

- **Nisarg Patel, Avesta Sasan, and Houman Homayoun. 2017. Analyzing hardware based malware detectors. In Proceedings of the 54th Annual Design Automation Conference 2017. ACM, 25.**

Patel et al. evaluated eleven different machine learning classification algorithms (BayesNet, NaiveBayes, SMO, SimpleLogistic, SGD, PART, OneR, MultiLayerPercep, Logistic, JRIP, J48) in terms of their accuracy and overhead to detect malware using values of hardware perfomance counters. Hardware implemenations (FPGA based) of these algorithms are also tested. It is shown that the rule and tree based classification algorithms are more efficient and have less area overhead. The evaluation further shows that the classifier (using branch instruction counter as input only) is the most "cost-effective hardware solution" for detection of malware.

- **Khaled N Khasawneh, Meltem Ozsoy, Caleb Donovick, Nael Abu-Ghazaleh, and Dmitry Ponomarev. 2015. Ensemble learning for low-level hardware-supported malware detection. In International Workshop on Recent Advances in Intrusion Detection. Springer, 3–25.**

Khasawneh et al. made use of ensemble learning techniques to combine Logistic regression (LR) and Neural Network (NN) based malware detectors. Combining the decision of multiple detectors reduces "false positive rate by half compared to" the use of a single detector. Different features related to instruction mixes, memory references and other architectural events collected for a set of benign and malware applications are used to train ML based malware detectors. Experimental evaluation shows 16.6 times reduction in overhead for online detection in comparison to software based detector and 8 times "improvement in relative detection speed". Implementation of the proposed malware detector on an FPGA based platform indicates minimal hardware implementation overhead.

- **Khaled N Khasawneh, Nael Abu-Ghazaleh, Dmitry Ponomarev, and Lei Yu. 2017. RHMD: evasion-resilient hardware malware detectors. In Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture. ACM, 315–327.**

Khasawneh et al. also used Neural networks and Logistic regression (LR) based hardware malware detectors to prove the possibility of evading malware detection. Availability of malware detector training data makes it possible for attackers to reverse engineer detectors and potentially modify malwares to evade their detection. They proposed randomization based technique to attain resilient malware detection and avoid reverse engineering.

- **Marco Chiappetta, Erkay Savas, and Cemal Yilmaz. 2016. Real time detection of cache-based side-channel attacks using hardware performance counters. Applied Soft Computing 49 (2016), 1162–1174.**

Chiappetta et al. proposed machine learning based technique to detect cache-based side-channel attacks. They trained Neural Networks based on collected performance counters values (instruction and cache related events) for benign and malicious applications (resposible for attacking through side-channels). The results show that the proposed technique can detect spy processes performing Flush+Reload type of side-channel attacks with very high accuracy.
