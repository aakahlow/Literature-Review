# Summaries of Papers Related to the Use of Machine Learning in Computer Architecture


## Microarchitecture Related 

### Branch Prediction



- **Towards a high performance neural branch predictor, Lucian N Vintan and Mihaela Iridon, International Joint Conference on Neural Networks, 1999.**

Vinton and Iridon did one of the earliest works in dynamic branch predictions using machine learning. 
They used neural networks with Learning Vector Quantization (LVQ) as a learning algorithm for neural networks and were able to achieve around 3% improvement in misprediction rate compared to conventional table based branch predictors.

- **Neural methods for dynamic branch prediction, Daniel A Jiménez and Calvin Lin, ACM Transactions on Computer Systems (TOCS), 2002.**

Jimenez in this paper used neural network based components, perceptrons, to perform dynamic branch prediction. Perceptrons are concidered simple and more accurate in comparison to other neural learning methods. Each single branch is allocated a perceptron. The inputs to the perceptron are the weighted bits of the "global branch history shift register", and the output is the decision about branch direction. One big advantage of perceptron predictor is the fact that the size of perceptron grows linearly with the branch history size (input to the perceptron) in contrast the size of pattern history table (PHT) in PHT based branch predictors which grows exponentially with the size of branch history. Therefore, within same hardware budget, perceptron based predictor is able to get a benefit from longer branch history register. The proposed perceptron based global branch predictor achieved 36% less branch mispredictions compared to the most accurate branch predictor of that time (McFarling hybrid predictor) given the similar hardware resources.

One of the main differences between the work of Jimenez et al and Vinton and Iridon (above two papers) is that Jimenez et al used "only history register as perceptron predictor, whereas Vinton and Iridon Vinton and Iridon used the history register and the branch address as input values to LVQ and backpropagation neural predictors". LVQ has complex hardware implementation due to computations involving floating point numbers, which can significantly increase the latency of
the predictor. In contrast, predictor proposed by jimenez et al used simpler training algorithm which can be implemented more efficiently. 

- **Piecewise linear branch prediction, Daniel A Jiménez, ACM SIGARCH Computer Architecture News, 2005.**

One drawback of the perceptron based branch predictor introduced by Jimenez et al (summarized above) was its inability to learn the behavior of linearly inseparable branches. A boolean function is "linearly separable" if all false instances of the function can be separated from its all true instances using a hyperplane. As an example XOR is linearly inseparable and AND is linearly separable. Jimenez later (in this paper) presented piecewise linear branch predictor. This branch predictor uses a set of piecewise linear functions to predict the outcomes for a single branch. These linear functions refer to distinct historical path that lead to the particular branch instruction. Graphically, all these functions, when combined together, form a surface. In contrast to the above perceptron based predictor, this predictor can learn the behavior of linearly inseparable branches (for example it can learn the behavior of XOR function).


- **Two-level branch prediction using neural networks, Colin Egan, Gordon Steven, Patrick Quick, Rubén Anguera, Fleur Steven, and Lucian Vintan, Journal of Systems Architecture, 2003.**

Later Egan et al in this work extended the work of Vinton and Iridon and used neural networks (based on Learning vector quantization and backpropagation) as a replacement for "second level PHT" in two-level predictors while retaining the "first level history register". They demonstrated a prediction accuracy comparable to two-level predictors of the time.

- **Dynamic feature selection for hardware prediction, Alan Fern, Robert Givan, Babak Falsafi, and TN Vijaykumar, ECE Technical Reports, Purdue University, 2000.**

This work introduced decision tree based branch predictor. Decision trees allow the branch predictor to be controlled by many "processor state features". The relevant features could change at run-time without increasing linearly in size with addition of new features (compared to table based predictors), providing significant benefits over convential table based predictors. 


- **SVMs for Improved Branch Prediction, Benjamin J Culpepper and Mark Gondree, Technical Report, UC Davis, 2005.**

Culpepper and Gondree used support vector machines (SVM) to improve accuracy of branch prediction. This SVM based branch predictor performed better compared to fast-path based predictor and gshare predictor at high hardware budgets. 

- **A Study on Deep Belief Net for Branch Prediction, Yonghua Mao, Junjie Shen, and Xiaolin Gui, IEEE Access, 2017.**

Mao et al. applied deep learning to branch prediction problem. The specific deep neural network used is Deep Belief Networks (DBN), which are easy to train. It is shown that the DBN predictor (based on offline learning) can reduce misprediction rate by 3-4 % on average depending on the benchmark type compared to perceptron based predictor. It is shown that the most of the benefits for DBN predictor come from its ability to reduce misprediction rate for linearly inseparable branches which perceptron being a linear classifier could not learn. It is also shown that DBN based predictor does not perform better than the TAGE predictor (considered as the best predictor till date) in most of the cases.

### Memory Scheduling

- **Self-optimizing memory controllers: A reinforcement learning approach, Engin Ipek, Onur Mutlu, José F Martínez, and Rich Caruana, ISCA, 2008.**

Ipek et al. introduced reinforcement learning based DRAM scheduling. The proposed memory controller increased utilization of memory bandwidth and showed a speed-up of 19\% for tested applications. The DRAM scheduler which acts as a reinforcement learning agent utilizes system state defined by different factors like number of read/write requests residing in the transaction queue. The actions that the agent can take include all normal commands of DRAM controller like read, write, precharge and activate. Each action results to an immediate reward and the agent (DRAM scheduler) eventually learns a policy to maximize the "cumulative long-term reward". Q-values often associated with credit assignment define what will be the eventual benefit of any action under the given state. Q-values are stored against each action-state pair and the agent tries to pick the action which will lead to the largest Q-value. Number of Q-values against each action-state pair required to store can become very lage. A learning model CMAC is used to store all Q-values to save space. A five stage hardware pipeline is introduced to calculate Q-values each processor cycle. 

- **MORSE: Multi-objective reconfigurable self-optimizing memory scheduler, Janani Mukundan and Jose F Martinez, HPCA, 2012.**

Later, Mukundan and Martinez used genetic algorithms to propose MORSE (Multi-objective Reconfigurable Self-optimizing Memory Scheduler) extending Ipek et al's work. MORSE can target optimization of different metrics like performance, energy and throughput.

- **Transactional Memory Scheduling Using Machine Learning Techniques, Basem Assiri and Costas Busch, Euromicro International Conference on Parallel, Distributed, and Network-Based Processing (PDP), 2016.**

Assiri and Busch used ML algorithms to improve the performance of schedulers for transactional memories. Transactional memories are used in multicore processors to improve performance by "avoding thread synchronization and locks overhead". Assiri and Busch have improved the working of Lazy Snapshot Algorithm (an algorithm for transactional memory scheduling) using different Machine Learning algorithms like K-NN, SVM and markov models. The evaluation shows that the K-NN (K-Nearest Neighbor) performs the best (in terms of accuracy and suitability) out of the studied ML algorithms.

### Prefetching Techniques

- **Machine learning-based prefetch optimization for data center applications, Shih-wei Liao, Tzu-Han Hung, Donald Nguyen, Chinyen Chou, Chiaheng Tu, and Hucheng Zhou, In Proceedings of the Conference on High Performance, Computing Networking, Storage and Analysis, 2009.**

Liao et al. built a framework which could figure out the best possible configuration of prefetchers for data-center workloads using different machine learning algorithms like KNN, SVM, decision trees and logistic regression. The proposed framework was applied to hardware prefetchers in an Intel Core 2 CPU. Framework's predicted prefetcher configuration achieved close to optimal performance.

- **Maximizing hardware prefetch effectiveness with machine learning, Saami Rahman, Martin Burtscher, Ziliang Zong, and Apan Qasem, International Conference on High Performance Computing and Communications, 2015.**

Rahman et al. built a framework using logistic regression and decision trees to identify the best prefetcher configuration for given multithreaded code (Liao et al's work focused on serial applications only). Hardware prefetcher configuration guided by the presented machine learning framework achieved close to 96\% speed-up of optimum configuration speed-up.

- **Data Cache Prefetching with Perceptron Learning, Haoyuan Wang and Zhiwei Luo, arXiv preprintarXiv:1712.00905, 2017.**

Wang and Luo proposed perceptron based data cache prefetching. The proposed prefetcher is a two level prefetcher which uses conventional table-based prefetcher at the first level. At the second level a perceptron is used to reduce unnecessary prefetches by identifying memory access patterns. The quantified decision result of first level alongwith some information about the cache line is transferred to the second level. A set of features (that correlate with the decision of prefetching) is generated and it forms the input to the perceptron. The features include: prefetch distance, transition probability, "Block address bits XOR program counter", occurrence frequency and fixed input. Experimental evaluation using SPEC-CPU2006 benchmarks shows that the proposed prefetcher is able to achieve "60.64\%-83.84\%" less memory prefetch requests.


- **Semantic locality and context-based prefetching using reinforcement learning, Leeor Peled, Shie Mannor, Uri Weiser, and Yoav Etsion, ISCA, 2015.**

Peled et al. used reinforcement learning to approximate program semantic locality, which was later used to anticipate data access patterns to improve prefetching decisions. Peled et al. realized that the use of irregular data structures can result in lower temporal and spatial locality. They relied on program semantics to capture data access patterns, and used reinforcement learning to approximate semantic locality. Accesses are considered to have semantic locality if there exists a relationship between them via a series of actions. Specifically program semantics are approximated by the use of a reinforcement learning algorithm. A subset of different attributes (e.g. program counter, accesses history, branch history, status of registers, types and offsets of objects in program data structures and types of reference operations) is used to represent the current context a program. The current context is used to make prefetching predictions and the reinforcement learning algorithm finaly updates the weights of contexts depending on the usability of current prefetches. Prefetcher is implmeneted in gem5 simulator and an LLVM pass is used to generate program semantic information. Results show that the prefetcher can achieve more than 32\% improvemnt in performance beating the competing prefetchers.


- **Towards Memory Prefetching with Neural Networks: Challenges and Insights, Leeor Peled, Uri Weiser, and Yoav Etsion, arXiv preprint arXiv:1804.00478, 2018.**

Later Peled et al. used neural networks to capture semantic locality. Memory access streams alongwith a machine state is used to train neural network at run-time which predicts the future memory accesses. Evaluation of the proposed neural prefetcher using SPEC2006, Graph500 benchmarks and other hand-written kernels indicated an average speed-up of 22\% on SPEC2006 benchmarks and 5x on other kernels. Peled et al. also performed a feasibility analysis of the proposed prefetcher which shows that the benefits of neural prefetcher are outweighed by other factors like learning overhead, power and area efficiency. However, with a few more advancements in Neural network technology such overheads can be avoided making neural prefetchers more realizable.

- **Block2Vec:A Deep Learning Strategy on Mining Block Correlations in Storage Systems, Dong Dai, Forrest Sheng Bao, Jiang Zhou, and Yong Chen, 2016 45th International Conference on Parallel Processing Workshops (ICPPW).**

Dai et al. exploited deep learning techniques to propose \textit{Block2Vec} (which is inspired by Word2Vec used in word embeding), which can find out correlations among blocks in storage systems. Such information can be used to predict the next block accesses and used to make prefetching decisions.~They introduced a new vector based representation of blocks which include a number of features that define the block.~The correlation among blocks can be found out by using the distance between block vectors. Block2Vec provides two different models to choose for training purposes. CBOW (Continuous Bag-of-Words) predicts the current block given past and future blocks and Skip-gram model predicts the past and future blocks given the current block. Block2Vec also considers the clossness in time as a feature to impact the training process to determine the block correlations. The Skip-gram model of is shown to have the best next access block prediction accuracy when compared with other accepted methods like PG (Probability Graph) and SP (Sequential Prediction).

### Cache Line Reuse

- **Neural methods for dynamic branch prediction, Daniel A Jiménez and Calvin Lin, ACM Transactions on Computer Systems (TOCS), 2002.**

Jimenez et al. took help of genetic algorithms to introduce a pseudo-LRU (least recently used) insertion and promotion machanism for cache blocks in last level caches, which could result in ~5\% speedup compared to traditional LRU (least recently used) algorithm using much less overhead. The performance of the proposed mechanism matched other contemporary techniques like DRRIP (dynamic rereference interval prediction) and PDP (protecting distance policy with less overhead.

- **Perceptron learning for reuse prediction, Elvira Teran, Zhe Wang, and Daniel A Jiménez, MICRO, 2016.**

Teran et al. applied perceptron learning algorithm (not actual perceptrons) to predict reuse of cache bkocks. Different features (like addresses of recent memory instructions and portions of address of current block) are hashed and xored with the program counter to generate an index into a counters' table. This table has weights for each feature which are then cumulated. If the sum of the weights exceed a threshold the current block is predicted to be not reused. On correct prediction, the weights are increased and they are decreased if prediction is incorrect. The proposed method is shown to have less false positives compared to other methods that results in performance improvements. This method employs various features and has a better chance of working as correlation of features
with reuse may vary across the program. However, its hardware complexity is higher compared to the other modern reuse predictors.

### Cache Configuration

- **On the finding proper cache prediction model using neural network, Songchok Khakhaeng and Chantana Chantrapornchai, In 8th International Conference on Knowledge and Smart Technology (KST), 2016.**

Khakhaeng and Chantrapornchai used perceptron based neural network to build model to predict ideal cache block size. Neural network is trained using features from address traces of NU-MiBench suite. The particular features used for training include: cache misses and size and frequency adjoining addresses which reflects temporal and spatial locality of the program. Using a cache simulator (SMPCache) and selected data mining applications from NU-MineBench suite, address traces are collected.  Neural network is trained by keeping most of the cache configuration fixed other than the block size.  

- **A Machine Learning Methodology for Cache Recommendation. Osvaldo Navarro, Jones Mori, Javier Hoffmann, Fabian Stuckmann, and Michael Hübner, International Symposium on Applied Reconfigurable Computing, 2017.**

Navarro et al. proposed a ML based methodolgy to determine the optimal cache configuration given any input application considering its effects on energy and performance. First several benchmarks profiled to collect different features like cache hits, misses, execution time and energy consumption using different cache configurations. Dynamic instruction sequences (n-grams) are generated from the programs and are used as input features for training of different classifiers. Different types of classifiers like Bayes, Functions, Lazy, Meta, RUles and Trees (modify) are used. These classifiers then map given inputs to output classes (i.e. the optimal cache configuration). The evaluation of the trained classifiers show a precision of above 99\%.

### Value Prediction

- **Exploring perceptron-based register value prediction, John Seng and Greg Hamerly, In Second Value Prediction and Value-Based Optimization Workshop, held in conjuction with ASPLOS, 2004.** 

Seng and Hamerly did one of the earliest works to present perceptron based register value predictor. Perceptron based register value predictor maintains a table of perceptrons. Particular instruction address bits are used to index into that table of perceptrons and decide which perceptron to use. Each perceptron is fed with the global history of recently committed instructions. Predictor makes an output of 1 (if the instruction is redundant i.e. it predicts that register value produced by an instruction is already in the destination register or another register in register file) or -1 (if the instruction is not redundant). On comparing the prediction with the actual output, perceptron is fed with the positive or negative feedback depending on the accuracy of the prediction. Experimental evalution of the predictor showed that for given budget perceptron based value predictor performed better than saturating counter predictor. On average, 8KB perceptron based predictor showed a speed up of 8.1\%.

- **Neural confidence estimation for more accurate value prediction, Michael Black and Manoj Franklin, In Proceedings of the 12th international conference on High Performance Computing, 2005.**

Black and Franklin proposed perceptron based confidence estimator for a value predictor. Perceptrons identify the instructions that affect the accuracy of a prediction and estimate the confidence in the prediction. Comparison of the proposed perceptron based global estimator with the "conventional local confidence estimator" shows that it results into lesser mispredictions. As discussed earlier, perceptrons are unable to learn linearly inseparable functions. In this case, linear inseparability becomes an issue "if a correct prediction on a past instruction causes the current instruction to predict correctly sometimes and incorrectly at other times." 

### GPU Microarchitecture

- **Neural acceleration for gpu throughput processors, Amir Yazdanbakhsh, Jongse Park, Hardik Sharma, Pejman Lotfi-Kamran, and Hadi Esmaeilzadeh, MICRO, 2015.**

Yazdanbakhsh et al. used neural accelerators in GPUs to do approximation of GPU code to save energy. A neural network learns the behavior of a code segment
and executes itself on a neural hardware. A compiler is used to transform GPU code to neural equivalent. With quality degradation of 2.5\%, 2.1x reduction in energy consumption and 1.9x speed up is achieved.


### Performance and Power Estimation

Sherwood et al \cite{sherwood2002automatically} used K-means clustering to form groups/clusters of similar basic block vectors (BBV). A basic block vector (BBV) contains the frequency of occurence of all basic blocks (a basic block corresponds to a sequence of instructions with a single entry and a single exit) of a program during a particular interval of execution. As a first step, the dimension of Basic Block Vector (BBV) is reduced using random projection as the dimension of BBVs can become very large for the entire execution of the program. Then, K-means clustering algorithm is used to find groups of similar BBVs. Any distance metric (e.g. Manhattan distance) can be used to compute the distance amont the BBVs.  These groups then act as a few representative portions of the entire program and are known as \textit{Simpoints}. These simpoints can be used to approximate performance/power for the whole program.


Hoste et al \cite{hoste2006performance} detected similarity among programs using data from various microarchitecture-independent statistics (like instruction classes, instruction-level parallelism and register traffic, branch predictability and working set sizes) to predict performance of programs similar to reference benchmarks with known performance on specific microarchitecture. Three different techniques (Normalization, Principal Component Analysis (PCA) and Genetic Algorithms) are used to relate program similarity to performance prediction. %These techniques are applied on data matrices containing previously mentioned microarchitecture-independent characteristics for a particular benchmark which may or may not contain the performance measures. 
Considering the benchmarks in the vicinity of application of interest, it's performance is estimated using the previously mentioned techniques. The technique of normalization normalizes all microarchitecture independent characteristics such that their mean is zero and variance is 1 to avoid higher weights for characteristics with larger values. Assigning higher weights for characteristics with larger values could lead to incorrect conclusion if euclidean distance is used to figure out vicinity of application of interest in benchmark space. The second technique used is an
uncorrelated data extraction technique Prinicpal Component Analysis (PCA), which tries to isolate uncorrelated microarhcitecture independent characteristics as correlated characteristics would be given bigger weights by euclidean distance. The third used technique is a proposed Genetic Algorithm that considers the importance of different characteristics opposed to previous two techniques which treat all normalized charactersittics or normalized principal components with same importance while calculating euclidean distance. %The genetic algorithm assigns a scaling factor to different characterisitcs depending on their importance while calculating the euclidean distance. 
The evaluation of the three techniques using SPEC-CPU2006 benchmarks shows that the genetic algorithm is the most accurate one to estimate program performance compared to the other ones.
%Using SPEC-CPU2006 benchmarks, the evaluation shows genetic algorithm to be more accurate compared to the other two techniques.

Yount et al \cite{elmoustapha2007comparison} compared various machine learning (ML) algorithms with respect to their ability to analyze architectural performance of different workloads and found tree-based ML models to be the most interpretable and almost as accurate as Artificial Neural Networks (ANNs). Workload performance metrics collected on an Intel Core2 Duo system train following ML algorithms: Multi-Linear Regression, Aritificial Neural Networks (ANNs), Locally Weighted Linear Regression, Support Vector Machine (SVM) and Model Trees (M5). M5 which is a tree based ML model shows better results compared to the other techniques in terms of accuracy and interpretability. Different microarchitectural miss events related to instruction types, memory, branch predictor and TLBs normalized by the number of exeucted instructions are used to train chosen ML techniques to predict cycles per instruction (CPI) values for a particular interval of execution. 10-fold cross validation \cite{james2013introduction} is then used to assess the accuracy of generated models. All these techniques show error %relative absolute error 
below 10\% and have high correlation coefficients. M5's accuracy is little less than that of ANNs but it is the most interpretable technique out of the studied ones. ANNS, LWR and SMOreg (SVM based model) being black box techniques do not provide enough interpretability of the output model and are not very useful in determining the individual effects of different miss-events on performance. Multi-linear regression has better interpretability, however it cannot exhibit the non-linear effects generated by interactions of different microarchitectural events used as predictors (input variables).

Lee et al \cite{lee2015powertrain} proposed ML based calibration methodology called \textit{PowerTrain} to improve accuracy of McPAT (power estimator simulator) using regression analysis. McPAT is a well-known power estimation simlator, but it is shown to have various inaccuracies due to different sources like unmodeled architectural components, discrepancy between the modeled and actual hardware components and vague configurational parameters \cite{xi2015quantifying}. The proposed calibration methodology called \textit{PowerTrain} uses power measurements on real hardware to train McPAT using regression analysis. In a two step process, the first step involves generating a regression coefficient to move total estimated power (by the model) close to the hardware power observed empirically. The second step works at lower granularity at block-level and generates coefficients for each block separately such that the overall predicted power can move closer to the actual power. Separate coefficients for static and dynamic power are generated. Non-negative least square solver \cite{koh2007interior} is used to reduce least mean square error as normal least mean square error (LMSE) solvers (like simple linear regression) can generate negative coefficients, which might reduce the interpretability of the power equation.~Using an ARM-A15 processor based ODROID-XU3 development board and benchmarks from MiBench \cite{guthaus2001mibench} and SD-VBS \cite{venkata2009sd} suites for training and testing purposes, the authors have shown that the proposed calibration methodology can result into 4\% MPAE (Mean Percentage Absolute Error) compared to the 5.29\% MPAE of the baseline.


Walker et al \cite{walker2017accurate} used various perormance monitoring counters and ML algorithms to estimate power consumption for mobile and embedded devices. Various performance monitoring counters counting different architectural and microarchitectural events are read. Heirarchical cluster analysis \cite{bridges1966hierarchical} is used to group different counters/events into clusters. Only events from those clusters are selected which seem to correlate highly with the consumed power. \textbf{[Vague]} Regression analysis is combined with the other understanding of the way the CPU consumes power to build a smart model. The values of all events are multiplied by $V^2$f to capture the relationship between power and other events before performing multiple linear regression using OLS estimator \cite{kutner2004applied}. A dynamic power component which does not depend on PMC events is also added to the model. Experiments performed using different workloads show average error of 2.8\% and 3.8\% for ARM Cortex-A15 and Cortex-A7 processors to estimate power. It is also shown that the proposed stable model can achieve less percentage error to estimate power compared to other techniques like \cite{pricopi2013power,walker2015run,rethinagiri2014system,rodrigues2013study}.

Reddy et al \cite{reddy2017empirical} studied correlation among gem5 \cite{binkert2011gem5} statistics and hardware performance monitoring counters to build a model in gem5 to estimate power consumption of ARM Cortex-A15 processor. This work uses the same model built by Walker et al \cite{walker2017accurate} but does not include all performance monitoring events as some gem5 equivalents would not be available. The used events in ML model are cycle counts, speculative instructions, L2 cache accesses, L1 instruction cache accesses and memory bus reads. It is shown that the differences between statistics of the simulator (gem5) and those of the real hardware only effect the "estimated power consumption by 10\%" [check numbers].

Lee et al \cite{lee2008cpr} used spline-based regression to build model for multiprocessor performance estimation from uniprocessor models to mitigate the problems associated with cycle accurate simulation of multiprocessors. The proposed technique is called "Composable Performance Regression" (CPR). The proposed technique also accounts for contention and penalty models in multiprocessor systems while estimating the performance of multiple workloads running on a multiprocessor with shared resources (such as last level cache). Their technique first estimates the performance of each single workload and then also estimates the contention behavior when a single workload runs in the presence of other workloads. Penalty model is used to combine both single-workload's performance with the estimation of possible contention for the particular workload. The experiments show median errors of approximately 4 and 6\% while predicting dual core and quad core performance from single core performance values.

Pricopi et al \cite{pricopi2013power} used regression analysis to propose an analytical model to estimate performance and power for heterogeneous multicore processors (HMPs). During an application run, a CPI (Cycles per Instruction) stack is built for the application using different microarchitectural events that can impact the execution time of the application. Relying on some compiler analysis results alongwith the CPI stack, performance and power can be estimated for other cores in an HMP system. Experiments performed with an ARM big.Little system indicate an intra-core prediction error below of 15\% and an inter-core prediction error of below 17\% \textbf{[Figure]}.

Zheng et al \cite{zheng2015learning} used Lasso Linear Regression and Constrained Locally Sparse Linear Regression (CLSLR) to explore correlation among performance of same programs on different platforms to perform cross-platform performance prediction. The test case used in their work is prediction of an ARM ISA platform based on performance on Intel and AMD x86 real host systems. Benchmarks from ACM programming contest \cite{ACMProg} are used for training purposes. Training set is profiled and different feature vectors containing count of various events like cache misses, branch misses etc. are collected using hardware performance counters on the host platform. These multi-dimensional vectors are fed to principal component analysis (PCA) to reduce their dimensionality. To map these feature vectors to target platform performance measurements techniques of Lasso Linear Regression and Constrained Locally Sparse Linear Regression (CLSLR) are used. The reason for using Lasso in place of ordinary linear regression is it's sensitivity to measurement noise and potential to overfit. Since, non-linear relationship can also exist between host feature vectors and target performance measurements, CLSLR is introduced to take care of such non-linearities. Applyig 10-fold cross-validation shows more than 15\% error for Lasso and less than 1\% error for CLSLR suggesting existence of non-linear relationship. Prediction accuracy is evaluated using some MiBench \cite{guthaus2001mibench} and SD-VBS (San Diego vision benchmark suite) \cite{venkata2009sd} benchmarks showing more than 90\% accuracy on average. The authors extended this work in \cite{zheng2016accurate} by proposing \textit{LACross} framework which applies similar methodology to predict performance and power at fine granularity of phases. \textit{LACross} is shown to have an average error less than 2\% in entire program's performance estimation, in contrast to more than 5\% error in \cite{zheng2016accurate} for SD-VBS benchmark suite \cite{venkata2009sd}.


Jundt et al. \cite{jundt2015compute} used a ML model named Cubist \cite{Cubist}, which uses a tree of linear regression models to observe the importance
of architectural blocks that have an effect on performance and power of high performance computing applications on Intel's Sandy Bridge and ARMv8 XGene processors.~All available hardware performance counters' data is collected on both platforms. To reduce the number of input varibales for the machine learning model, Principal Component Analysis and K-means clustering are used. N element vector corresponding to an input variable (composed of performance events values: examples) is computed which contains its principal components. Different clusters of input variables are found using K-means clustering. Out of each cluster only one variable is selected for further processing. These filtered input counters are used to train Cubist \cite{Cubist}, which generates a set of rules. The rules are generated based on training data which decide which regression model to follow given any test data. A diverse set of applications inlcuding NAS \cite{bailey1991parallel}, PARSEC \cite{bienia2008parsec}, SPEC-CPU2006 \cite{spec2006Web} and other suites is used. The trained models are shown to have a median error of 1.26\% and 7.36\% for x86 and ARM processors in predicting performance and an error of 13.9\% and 15.4\% for ARM and x86 processors to predict power. The results futher indicate that the biggest impact on performance is shown by the CPU frontend and branch predictor, while frontend and cache events carry the biggest effect for x86 processor. For power, memory and frontend events are the most important events for ARM and cache events have the biggest impact for x86 processor.

Mariani et al \cite{mariani2017predicting} used Random Forest to estimate HPC (High Performance Computing) applications' performance on clouds using hardware independent profiling of applications. Features related to types of executed operations, instruction level parallelism (ilp), register and memory traffic and library calls are collected for different parallel applications. These statistics are collected by running instrumented code which is generated by an LLVM based profiler. Features with constant values are excluded from the training model and only one of the correlated features is selected to add in the model. Performance is also measured for each application on specific cloud configuration generating tuples T(a,d,c) where a is the executed application, d is the dataset , c is the configuration and T is the wall-clock time for the execution of application. MPI implementations of NAS parallel benchmarks are used for experimentation. A statistical technique "Design of experiments" (DOE) \cite{eriksson2000design} is applied to the training data to remove inheret noise of the cloud environment. Random forests (RF) \cite{breiman2001random} is used to train the model because of its ability to work well when the number of given features is large. %RF is a set of different regression trees, where each tree inlcudes a subset of the training data. 
RF selects the features considering if they would lead to reduction in the error of the trained model. Experiments show a mean relative error of less than 15\% when the model is validated.

Wu et al \cite{wu2015gpgpu} proposed a neural network based GPU performance and power prediction to model solve the problem of slow simulation speed of simulators to study performance/power of GPUs. The proposed model estimates performance and power consumption of applications with changes in GPU configurations. Different OpenCL kernels running on a base configuration generate performance/power numbers alongwith other performance related counter values. All these values and the used configuration act as an input for training of the ML model. Two phase ML model is used. The first phase deals with grouping of kernels using K-means clustering algorithm. Scaling surfaces which define a cluster define how the change in configuration (number of computer units, engine and memory frequency) will impact kernel's performance. The next phase involves building of classifiers to estimate the correct cluster destination for a new kernel. These classifiers are based on neural networks. The inputs to the neural network are normalized performance counter values. The events counted by the used counters include vector and scalar instruction numbers, information related to read/write bandwidth, cache statisitcs, time of processing for vector and scalar instructions and number of load/store bank conflicts. The outputs of the neural network determines which cluster the given kernel belongs to. Experimental evaluation is performed by running OpenCL kernels from different suites like ParBoil \cite{stratton2012parboil}, Rodinia \cite{che2009rodinia}, SHOC \cite{danalis2010scalable}, OpenDwarfs \cite{feng2012opencl} and Phoronix \cite{} test suite on AMD Radeon GPU for testing purposes. 448 configurations (with eight different compute unit values, eight engine frequencies and seven memory frequencies) are used to collect counter values to train the proposed model. Then each configuration is considered as a base configuration for the testing purposes (leaving the other configurations as test points). [vague] The results shows 15\% average error in perfomance estimation and 10\% error in power estimation for a 3.3 times range of frequency, 2.9 times range of bandwidth and 8 times difference of cluster units \textbf{[FIGURE]}.

O'Neal et al \cite{o2017gpu} used various linear/non-linear regression algorithms to propose GPU performance predicting model focusing on "pre-silicon design" of GPUs using DirectX 3D workloads.~RatSim a functional simulator is used to run programs whose outputs (program counter values) are fed to build a model, which predicts the performance of the applications. GPUSim \cite{bakhoda2009analyzing} is used to generate the performance numbers (cycles per frame - CPF) needed for training of the ML model. Performed experiments use Intel Skylake GT3 GPU as a reference model which is modeled on GPUSim. Since, authors objective is to copy the entire pre-silicon design phenomena, already available GPU models are not used. The inputs to training model is a collection of program counters for each single workload. And the output of the model is a single CPF (cycles per frame) value for every program. 14 different linear and one non-linear regression models are used from diffrerent categories namely: Non-Negative Least Squares (NNLS), Ordinary Least squares (OLS), Regularization, Regularization-NNLS and Random-Forest (only non-linear). Random Forest comes out to be the most accurate model out of all the studied ones. 10-cross validation indicates an "out-of-sample error" of approximately 14\% for Random Forest.



Baldini et al \cite{baldini2014predicting} trained two binary classifiers Nearest Neighbor with Generalized exemplars (NNGE) and Support Vector Machine (SVMs) to predict
possible GPU speed-up given a run of parallel code on CPU (using OpenMP implementation). The adopted methodology dynamically profiles programs and collects a number of useful run-time features \textit{[VAGUE]} belonging to different categories like computation (arithemetic and SIMD operations), memory (load/stores) and control flow etc. GPU execution time and OpenMP execution time are also collected. This data is then used to train the previously mentioned binary classifiers (NNGE and SVM). The performed experiments using Parboil \cite{} and Rodinia \cite{} benchmarks suites on Intel Xeon processor with two graphics cards (ATI FirePro and Nvidia Tesla) indicate there is very little correlation between OpenMP speedup over serial execution and GPU speedup over serial exeuction \textbf{LOOK}. This means that the ... Moreover, the trained models can predict if the GPU implementation will be beneficial or not with an accuracy of 80\% approximately. It is also shown that using different trained classifiers with SVM (trained for different threshold speedups), the exact speed-up of GPU code can be predicted with 77-90\% accuracy.

Ardalani et al \cite{ardalani2015cross} proposed \textit{XAPP} (Cross Architecture Performance Prediction) technique to estimate GPU performance from CPU code using regression and bootstrap aggregating (discussed in section \label{sec:bst}). Different features associated with the program behavior are used to train machine learning models. These features include some basic features related to ILP (instruction level parallelism), floating point operations and memory opeations and also some advanced features like shared memory bank utlization, memory coealescing and branch divergence. This work follows a two-level ML approach. At the first level, regression is used to build the base model. Features are added to this base model one by one and only those features are added which improve performance of the model above a threshold (known as the technique of "forward feature selection stepwise regression"). At the second level, ensemble prediction ("bootstrap aggregating") is used. Using applications from different benchmark suites like PARSEC \cite{bienia2008parsec}, Parboil \cite{stratton2012parboil}, NAS \cite{bailey1991parallel} and Rodinia \cite{che2009rodinia} for training and testing of the proposed model, it is shown to have a gemetric mean error of approximately 11\% (average error of 22\%). Model is trained on Nvidia's Maxwell GPU and validated on Nvidia' Kepler GPU.


